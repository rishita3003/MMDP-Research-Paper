{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfaef9dd",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; margin-bottom: 20px;\">\n",
    "  <img src=\"https://tse4.mm.bing.net/th?id=OIP.EhV6r5gkOCL2LvxAkFAnigAAAA&rs=1&pid=ImgDetMain\" alt=\"SimCLR Logo\" style=\"height: 60px; margin-right: 15px;\">\n",
    "  <div>\n",
    "    <h2 style=\"margin: 0;\">Look Twice, Learn Better : How SimCLR transformed Computer Vision</h2>\n",
    "    <p style=\"margin: 0; color: #0066cc; font-size: 22px;\"><em>Rishita Agarwal</em></p>\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a5cfc8",
   "metadata": {},
   "source": [
    "<iframe width=\"560\" height=\"315\" src=\"https://youtu.be/muJMTto75qE \" title=\"SimCLR Explanation\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368e2fa1",
   "metadata": {},
   "source": [
    "## Motivation behind this project topic?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3914928e",
   "metadata": {},
   "source": [
    "The growth of digital images has created a havoc in machine learning, we have a lot of visual data, yet supervised learning approaches act as a bottleneck due to the lack of high quality labeled data. This is one of the major challenges in computer vision today.\n",
    "\n",
    "One of the solution which can be proposed for this can be manual annotation of the data, but manual labelling is expensive, time consuming and many times impractical at large scale. Some times it may also require expert knowledge in a particular domain.\n",
    "\n",
    "**Unsupervised learning** offers a more promising path by leveraging the large amount of unlabeled data available. Though, it is not that easy to produce results as good as supervised learning. Some of the key requirements are -\n",
    "1. The learned representation should be **generalizable** to diverse downstream tasks as well.\n",
    "2. The approach should **scale computationally** with larger datasets and model sizes.\n",
    "3. Features should capture **meaningful semantic** information.\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"https://www.edushots.com/upload/articles-images/b35a6ab4259fcd2fa572cc62333ac5ec15371617.jpg\" alt=\"SimCLR Framework\" width=\"45%\" style=\"margin-right: 10px;\"/>\n",
    "    <img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20231213175718/Self-660.png\" alt=\"SimCLR Results\" width=\"45%\"/>\n",
    "</div>\n",
    "<p style=\"text-align: center;\"><em>Figure: Unsupervised Learning (left) and Self Supervised Learning (right)</em></p>\n",
    "\n",
    "SimCLR addressed these requirements in a simplistic manner, still managing to achieve state-of-the-art results, grabing my attention to this topic. The idea was simple yet innovative, which actually was derived from the essential components of existing methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37a398a",
   "metadata": {},
   "source": [
    "## History and current works?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8086c5c2",
   "metadata": {},
   "source": [
    "The initial methods relating to **self supervised visual representations** led to this exploration. Let me talk more about these methods.\n",
    "\n",
    "It all started with **handcrafted pretext** tasks like predicting image rotation or colorizing grayscale images. These methods were a good start to the finding a good representation, but it was noticed that the learnt representations were more specific to the pretext task rather than being general purpose.\n",
    "\n",
    "Then, approaches like **InstDisc and CPC** introduced the concept of contrastive objectives (Fig. given below) but still relied on complex architectures or memory banks to store representations.\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"https://insights.willogy.io/assets/static/contrastive_learning_intuition.42db587.208b1cf168018c6226966d0407c62134.jpg\" alt=\"Contrastive Learning Concept\" width=\"50%\"/>\n",
    "</div>\n",
    "<p style=\"text-align: center;\"><em>Figure: Visualization of contrastive learning approach</em></p>\n",
    "\n",
    "Finally **SimCLR** simplied the approach of contrastive learning dramatically, showing that with right data augmentation, loss function and projection head, superior results with a straightforward framework could be achieved.\n",
    "Even after SimCLR more methods based on its insights were built, like, MoCo v2, SimSiam etc. to further improve the self supervision, trying to reduce the need for negative examples. CLIP and CLAP are 2 multimodal approaches to Contrastive learning, applying CL to image-text and audio-text pairs. I will give a basic introduction to these methods towards the end of this blog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7e00cd",
   "metadata": {},
   "source": [
    "## Diving deep into Contrastive Learning and SimCLR's approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3693a4",
   "metadata": {},
   "source": [
    "**Contrastive learning** is an approach that learns representations by comparing similar and dissimilar samples. The fundamental idea behind this concept is: Similar items (\"Positive\") should be closer together and dissimilar items (\"Negative\") should be further apart.\n",
    "\n",
    "It is a self supervised technique which learns meaningful representation without any explicitly labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc63f276",
   "metadata": {},
   "source": [
    "**SimCLR (Simple Framework for Contrastive Learning of Visual Representation) Framework** \n",
    "\n",
    "This framework consists of 4 major components - \n",
    "1. **Data Augmentation Module** - In this module, each input image goes through stochastic augmentation twice, to generate 2 different views.\n",
    "\n",
    "``` \n",
    "X -> [augmentation] -> x̃ᵢ\n",
    "X -> [augmentation] -> x̃ⱼ\n",
    "```\n",
    "\n",
    "The augmentation sequence is as follows :\n",
    "- **Random Cropping** - After cropping the image randomly, it should again be resized to original size.\n",
    "- **Random color distortion** - Include color dropping, brightness, contrast etc.\n",
    "- **Random Gaussian blur** - Gaussian function to smooth an image. The effect is similar to viewing the image through a translucent screen, creating a hazy appearance by reducing image noise and detail.\n",
    "\n",
    "The most important combination of transformation is that of cropping (spatial transformation) and color distortion (appearance transformation). A good reason for this is, that without color distortion the networks can exploit the shortcut of matching color histograms, rather than actually learning semantic features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3023ed97",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"https://tse1.mm.bing.net/th?id=OIP.l9m-_lWHc2iopae_sHtdUwHaDM&rs=1&pid=ImgDetMain\" alt=\"Data Augmentation\" width=\"45%\" style=\"margin-right: 10px;\"/>\n",
    "    <img src=\"https://img-blog.csdnimg.cn/20201013134203801.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h6MTMwODU3OTM0MA==,size_16,color_FFFFFF,t_70#pic_center\" alt=\"Data Augmentation\" width=\"45%\" style=\"margin-right: 10px;\"/>\n",
    "</div>\n",
    "<p style=\"text-align: center;\"><em>Figure: Data Augmentation Example (left) and Composition of Augmentation Techniques (right)</em></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec0478",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "2. **Base Encoder Network**\n",
    "\n",
    "SimCLR employs a standard CNN (ResNet architecture) as its base encoder. Given an augmented image x̃, the encoder generates a representation vector -\n",
    "\n",
    "``` h = f(x̃) = ResNet(x̃) ```\n",
    "\n",
    "where h ∈ ℝᵈ is the output after the average pooling layer.\n",
    "\n",
    "3. **Projection Head**\n",
    "This head maps the representation h to a space z, where contrastive loss is applied -\n",
    "\n",
    "``` z = g(h) = W₂σ(W₁h) ```\n",
    "\n",
    "Where σ is a ReLU nonlinearity, and W₁ and W₂ are learnable weights. \n",
    "In this research paper, they have made a relatively simple MLP with one hidden layer as the projection head. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b793f0e",
   "metadata": {},
   "source": [
    "## Contrastive Loss and Where should it be applied?\n",
    "\n",
    "It was noticed that applying the contrastive loss directly on the representations h hurts their quality for downstream tasks. The projection head allows the model to discard information that may be useful for the downstream task but not for the contrastive task.\n",
    "\n",
    "Crucially, after training is complete, the projection head is discarded, and the **representation h is used for downstream tasks.**\n",
    "\n",
    "#### Contrastive Loss function \n",
    "A special form of contrastive loss called NT-Xent (Normalized Temperature scaled Cross Entropy Loss) is used. For a positive pair (i, j) :\n",
    "\n",
    "``` ℓᵢ,ⱼ = -log(exp(sim(zᵢ,zⱼ)/τ) / Σₖ₌₁²ᴺ 1[k≠i]exp(sim(zᵢ,zₖ)/τ)) ```\n",
    "\n",
    "Where:\n",
    "\n",
    "- sim(u,v) = u·v/‖u‖‖v‖ is the cosine similarity\n",
    "- τ is a temperature parameter that controls the concentration level of the distribution.\n",
    "- The sum is over all 2N examples in the batch (including the other augmented views)\n",
    "\n",
    "This loss effectively treats each augmented image in the batch as a single positive example (the other augmented view of the same image) and 2N-2 negative examples (all other augmented images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ad0f7a",
   "metadata": {},
   "source": [
    "#### *What's the temperature parameter?* \n",
    "Hmmm... Good question. Since we have already L2 normalized our embeddings z1 and z2, why do we need this temperature scaling parameter? The temperature parameter (τ) essentially controls how \"peaky\" or \"smooth\" the distribution becomes.\n",
    "\n",
    "1. **Lower temperature** - Amplifies the differences between the similarity scores and makes the model more certain about its choices. Eg. τ = 0.1, would tend to give sharper distinctions between positive and negative pairs and model would focus more on hardest negative pairs.\n",
    "2. **Higher temperature** - Smooths out the differences between similarity scores and reates a more uniform distribution. It allows the model to consider a broader range of negative examples\n",
    "\n",
    "So, normalization is needed to ensure all embeddings lie on the unit hypersphere (-1 to 1 for cosine similarity) and temperatures takes care of the concentration of attention on the positive and negative pairs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55d04e3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"https://sthalles.github.io/assets/contrastive-self-supervised/cover.png\" alt=\"SimCLR Framework\" width=\"70%\"/>\n",
    "</div>\n",
    "<p style=\"text-align: center;\"><em>Figure: SimCLR Framework</em></p>\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7056e264",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "The major points which can be reflected from this paper are described in the section.\n",
    "Based on the extensive ablation studies which were conducted -\n",
    "1. **Data augmentation composition is critical** - It was observed that following a specific sequence and combination of augmentations enhanced the results. Random cropping + color distortion provides significant benefit.\n",
    "\n",
    "2. **Normalized embeddings and temperature scaling matters** - Temperature effectively controls the importance of difficult negative examples. \n",
    "3. **Contrastive Learning benefits from larger batch sizes** - Larger batches (up to 8192) continue to improve performance as they provide more negative examples.\n",
    "4. **Longer training improves results** - SimCLR benefits more from training for longer periods than supervised counterparts.\n",
    "5. **Non linear projection head is crucial** - A linear projection is better than no projection, but non linear projection works even better, improving accuracy 3-4%.\n",
    "6. **Unsupervised learning benefits more from model scaling** - As the model size increases, the gap between supervised and self supervised models shrinks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07964fe7",
   "metadata": {},
   "source": [
    "## Algorithm in a nutshell\n",
    "\n",
    "1. Sample a minibatch of N images\n",
    "2. For each image x, generate two augmented versions x̃ᵢ and x̃ⱼ\n",
    "3. Compute representations ```h = f(x̃)``` and projections ```z = g(h)``` for all 2N augmented images\n",
    "4. For each positive pair, compute the contrastive loss using all other 2N-2 augmented examples as negatives\n",
    "5. Update the networks f and g to minimize the loss\n",
    "6. After training, discard the projection head g and use the encoder f and representation h for downstream tasks\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./blog_images/SimCLR.png\" alt=\"SimCLR Framework\" width=\"30%\"/>\n",
    "</div>\n",
    "<p style=\"text-align: center;\"><em>Figure: SimCLR Simple Example</em></p>\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a17670c",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The simplicity and effectiveness of SimCLR made it a pivotal development in self-supervised learning. Some notable achievements:\n",
    "\n",
    "- Linear classifiers trained on SimCLR representations achieved **76.5% top-1** accuracy on ImageNet, a *7% relative improvement* over previous state-of-the-art methods\n",
    "- With just 1% of labeled ImageNet data, fine-tuned SimCLR achieved **85.8% top-5** accuracy\n",
    "- SimCLR outperformed supervised pre-training on multiple transfer learning tasks\n",
    "\n",
    "Perhaps most importantly, SimCLR showed that with the right components, a simple approach to contrastive learning could outperform more complex methods, setting a new direction for self-supervised learning research.\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./blog_images/results.png\" alt=\"SimCLR Framework\" width=\"50%\"/>\n",
    "</div>\n",
    "<p style=\"text-align: center;\"><em>Figure: SimCLR results on transfer learning tasks</em></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f111b71",
   "metadata": {},
   "source": [
    "## What surprised me!?\n",
    "\n",
    "After going through the results and concept in depth, I found several aspects which surprised me -\n",
    "1. **The power of simple data augmentation:** A carefully composed and sequential data augmentations provided such effective contrastive tasks without any complex architecture. Random cropping + color distortion in this case. \n",
    "\n",
    "A question one might have is - **Why Random Cropping works??**\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial17/crop_views.svg?raw=1\" alt=\"SimCLR Framework\" width=\"50%\"/>\n",
    "</div>\n",
    "<p style=\"text-align: center;\"><em>Figure: SimCLR Simple Example</em></p>\n",
    "\n",
    "When performing randomly cropping and resizing, we can distinguish between two situations: (a) cropped image A provides a local view of cropped image B, or (b) cropped images C and D show neighboring views of the same image. While situation (a) requires the model to learn some sort of scale invariance to make crops A and B similar in latent space, situation (b) is more challenging since the model needs to recognize an object beyond its limited view. \n",
    "\n",
    "\n",
    "\n",
    "2. **Projection head paradox:** It was observed through experiments that the projection head works to remove information that may be useful for downstream tasks, which worked in a fascinating way as then the result of projection head could be used for contrastive loss.\n",
    "\n",
    "3. **Transfer learning performance:** This aspect highlights the major aim of the representations produced by SimCLR. It was observed that the SimCLR representations performed better than supervised ImageNet pretraining was completely unexpected and important aspect. This removed the issue of task specific representation of self supervision.\n",
    "\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"https://th.bing.com/th/id/OIP.tnTNLLEZNfrvlr_kkoBezwHaFO?rs=1&pid=ImgDetMain\" alt=\"Results\" width=\"50%\"/>\n",
    "</div>\n",
    "<p style=\"text-align: center;\"><em>Figure: SimCLR Results</em></p>\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c613dc5e",
   "metadata": {},
   "source": [
    "## Is there a scope for improvement??\n",
    "\n",
    "Even after all these fascinating aspects of this architecture, there still remains several areas for improvement: \n",
    "\n",
    "1. **Computational efficiency:** SimCLR requires large batch sizes and long training times for better performance, which makes it computationally expensive. Although, further works like *MoCo* address this issue using momentum encoders and memory banks.\n",
    "\n",
    "2. **Negative sample dependency:** The need for many negative examples require large batch sizes. Recent approaches like *BYOL and SimSiam* show that its possible to learn without explicit negatives.\n",
    "\n",
    "3. **Multimodality:** While SimCLR focusses on visual representations, *CLIP and CLAP* architectures extend it to multiple modalities, which yielded even richer representations.\n",
    "\n",
    "4. **Exploration:** Exploration of more achitectures other than ResNet and actual reasoning of why certain data augmentation compositions work better than other, would help develop self supervised learning more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f86594",
   "metadata": {},
   "source": [
    "## Want to see some results? \n",
    "#### *Lets take a look at a Pytorch implementation which I did for SimCLR*\n",
    "\n",
    "I implemented the SimCLR framework on a sample of *CIFAR10* (10000 images) for training the model, with *ResNet18* as the base encoder and a simple *MLP layer* for projection head.\n",
    "\n",
    "Each of the image is 32x32 color image and there are 10 different classes of images. The classes are -\n",
    "[Airplane, Automobile, Bird, Car, Deer, Dog, Frog, Horse, Ship]\n",
    "\n",
    "The code below applies data augmentation (Random Crop + Color distortion + Gaussian Blur) twice to the image -\n",
    "\n",
    "```python\n",
    "# Define data augmentation pipeline\n",
    "class TwoCropTransform:\n",
    "    \"\"\"Create two crops of the same image\"\"\"\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return [self.transform(x), self.transform(x)]\n",
    "\n",
    "# Define data augmentations as per SimCLR paper\n",
    "def get_simclr_pipeline_transform(size):\n",
    "    \"\"\"Return a set of data augmentation transformations as described in the SimCLR paper.\"\"\"\n",
    "    color_jitter = transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)\n",
    "    \n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomApply([color_jitter], p=0.8),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.GaussianBlur(kernel_size=int(0.1 * size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return data_transforms\n",
    "```\n",
    "\n",
    "On applying the above augmentation, I got 2N images for N images in my dataset. **Positive pairs** are those pair of images which we got from the same original image.\n",
    "\n",
    "The following are the various data augmentations on an image - \n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./blog_images/imp1.png\" alt=\"SimCLR Framework\" width=\"50%\"/>\n",
    "</div>\n",
    "<p style=\"text-align: center;\"><em>Figure: CIFAR10 image with augmentations</em></p>\n",
    "\n",
    "Then I defined the NT-Xent Loss (aiming to bring the positive pairs closer and the negative pairs further apart) -\n",
    "\n",
    "```python\n",
    "def forward(self, z_i, z_j):\n",
    "        # Get actual batch size (might be smaller than self.batch_size for last batch)\n",
    "        current_batch_size = z_i.size(0)\n",
    "        \n",
    "        # Create mask dynamically based on actual batch size\n",
    "        mask = self._get_mask(current_batch_size)\n",
    "        \n",
    "        # Concatenate the representations from the two augmentations\n",
    "        representations = torch.cat([z_i, z_j], dim=0)\n",
    "        \n",
    "        # Compute similarity matrix - more memory efficient approach\n",
    "        similarity_matrix = torch.mm(representations, representations.t())\n",
    "        \n",
    "        # Normalize the similarity matrix\n",
    "        sim_i_j = torch.diag(similarity_matrix, current_batch_size)\n",
    "        sim_j_i = torch.diag(similarity_matrix, -current_batch_size)\n",
    "        \n",
    "        positives = torch.cat([sim_i_j, sim_j_i], dim=0)\n",
    "        \n",
    "        # Remove diagonal (self-similarity)\n",
    "        mask_samples_from_same_repr = ~torch.eye(2 * current_batch_size, dtype=torch.bool, device=device)\n",
    "        negatives = similarity_matrix[mask_samples_from_same_repr].view(2 * current_batch_size, -1)\n",
    "        \n",
    "        # Scale by temperature\n",
    "        positives = positives / self.temperature\n",
    "        negatives = negatives / self.temperature\n",
    "        \n",
    "        # Create logits and compute loss\n",
    "        logits = torch.cat([positives.view(-1, 1), negatives], dim=1)\n",
    "        labels = torch.zeros(2 * current_batch_size, dtype=torch.long, device=device)\n",
    "        \n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss = loss / (2 * current_batch_size)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "```\n",
    "\n",
    "The similarity between the pairs of vectors is in the loss function. Thus, making use of the negative samples.\n",
    "\n",
    "The code below is the final code for training SimCLR -\n",
    "\n",
    "```python\n",
    "\n",
    "def train_simclr(model, train_loader, optimizer, criterion, epochs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    # Add debugging info\n",
    "    print(f\"Starting training with {len(train_loader)} batches per epoch\")\n",
    "    print(f\"Actual batch size: {next(iter(train_loader))[0][0].shape[0]}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for i, images in enumerate(train_loader):\n",
    "            try:\n",
    "                # Get the two augmented views of the same batch\n",
    "                (x_i, x_j) = images[0]\n",
    "                \n",
    "                # Add progress info\n",
    "                if i == 0 or (i+1) % 5 == 0:\n",
    "                    print(f\"Processing batch {i+1}/{len(train_loader)}\")\n",
    "                \n",
    "                # Debug info for first batch\n",
    "                if epoch == 0 and i == 0:\n",
    "                    print(f\"Input shape: {x_i.shape}\")\n",
    "                \n",
    "                x_i, x_j = x_i.to(device), x_j.to(device)\n",
    "\n",
    "                # Zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass for both augmentations with added memory efficiency\n",
    "                with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "                    z_i = model(x_i)\n",
    "                    z_j = model(x_j)\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    loss = criterion(z_i, z_j)\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                batch_count += 1\n",
    "                \n",
    "                # Free up memory\n",
    "                torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Record epoch loss\n",
    "        epoch_loss = running_loss / batch_count if batch_count > 0 else float('inf')\n",
    "        losses.append(epoch_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Processed {batch_count} batches\")\n",
    "        \n",
    "        # Save model checkpoint once at the end\n",
    "        if epoch == epochs - 1:\n",
    "            torch.save(model.state_dict(), f\"simclr_final.pt\")\n",
    "    \n",
    "    # Plot loss curve\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(losses)\n",
    "        plt.title('SimCLR Training Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.savefig('simclr_loss.png')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not plot loss curve: {e}\")\n",
    "    \n",
    "    return model, losses\n",
    "\n",
    "```\n",
    "Due to restricted time and computational resources, I took a subset of the data and also ran for a limited batch_size = 256 and 100 epochs. I took a temperature of 0.5 for a balanced check for positive and negative samples.\n",
    "\n",
    "Finally, training loss was a decreasing curve over the epochs: \n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./final_results/simclr_loss.png\" alt=\"SimCLR Framework\" width=\"50%\"/>\n",
    "</div>\n",
    "<p style=\"text-align: center;\"><em>Figure: Training Loss of SimCLR over the 100 epochs</em></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75d353d",
   "metadata": {},
   "source": [
    "### Results on downstream task "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad7b10d",
   "metadata": {},
   "source": [
    "#### 1. Linear Evaluation\n",
    "\n",
    "In this downstream task, I performed the linear evaluation task. Lets take a look at the steps involved in this: \n",
    "- Load your pre-trained SimCLR model\n",
    "- Freeze the encoder (so the weights don't change)\n",
    "- Extract features from all your training and test images\n",
    "- Train a simple linear classifier on top of these features\n",
    "- Evaluate performance on the test set\n",
    "\n",
    "The implementation extracts features from both training and test datasets in a single pass through the frozen encoder, significantly accelerating the evaluation process compared to end-to-end fine-tuning. The linear classifier itself is intentionally simple—just a single fully-connected layer mapping the 512-dimensional feature vectors to the 10 CIFAR-10 classes—to ensure that classification performance genuinely reflects the quality of the learned representations rather than the power of the classifier.\n",
    "This is the most common way to evaluate self-supervised representations because it tests if your features are linearly separable.\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./final_results/training_curves.png\" alt=\"Linear Evaluation\" width=\"45%\" style=\"margin-right: 10px;\"/>\n",
    "    <img src=\"./final_results/confusion_matrix.png\" alt=\"Confusion Matrix\" width=\"45%\" style=\"margin-right: 10px;\"/>\n",
    "</div>\n",
    "<p style=\"text-align: center;\"><em>Figure: Linear Evaluation Training Curves (left) and Confusion Matrix (right)</em></p>\n",
    "\n",
    "The above plots show that there is confusion between classes 0 and 8 (plane and ship) and classes 1 and 9 (car and truck). These can be as these are visually similar classes (similar shapes and features).\n",
    "\n",
    "The confusion patterns suggest that SimCLR has learned representations that capture some semantic similarities but still struggles with fine-grained distinctions between visually similar categories. This might be due to lack of training data as unsupervised learning needs more data in comparison to a supervised learning approach. Training on more epochs or increasing the batch size might also help improve the performance as then more negative samples can be compared.\n",
    "\n",
    "#### 2. Label Efficiency evaluation \n",
    "\n",
    "One of the big advantages of self-supervised learning is that it can work well with less labeled data. \n",
    "I implemented a ```evaluate_with_less_data()``` function that tests how well your model performs when trained with only a small percentage of labeled data (1%, 10%, 25%, etc.)\n",
    "The following is a plot showing how accuracy changes as you add more labeled data-\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./final_results/label_efficiency.png\" alt=\"Label Efficiency\" width=\"55%\" style=\"margin-right: 10px;\"/>\n",
    "</div>\n",
    "<p style=\"text-align: center;\"><em>Figure: Test accuracy on label efficiency task</em></p>\n",
    "\n",
    "The above plot show that there is a steep performance jump from **1% to 10% labeled data**. This accuraccy forms a plateau after 50% labelled data.\n",
    "\n",
    "The accuracy is not more than 50% right now, due to many factors including less training data used (only 10000 images) and low batch size and epochs for training. \n",
    "\n",
    "Still, the nature of the curve depicts that for this 50% accuracy only 10% of the lavelled data was enough. \n",
    "This confirms one of the core value propositions of contrastive learning methods like SimCLR - *they can learn useful representations that transfer well even with limited labeled data.*\n",
    "\n",
    "The relatively high performance **(43%)** even with just **1% labeled data** demonstrates the effectiveness of the knowledge transfer from self-supervised pre-training to downstream tasks.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b1fc5e",
   "metadata": {},
   "source": [
    "## Visualizing the representations\n",
    "\n",
    "The visualization shows how, after training, semantically similar images cluster together in the representation space, even though the model never saw class labels. This organization emerges purely from the contrastive learning objective.\n",
    "\n",
    "Given below is the t-SNE visualization of the embeddings -\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./final_results/tsne_embeddings.png\" alt=\"TSNE Visualization\" width=\"55%\" style=\"margin-right: 10px;\"/>\n",
    "</div>\n",
    "<p style=\"text-align: center;\"><em>t-SNE Visualization of the embeddings</em></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee8fd1f",
   "metadata": {},
   "source": [
    "The entire implementation is available at https://github.com/rishita3003/MMDP-Research-Paper/blob/main/training_evaluation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0f4499",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "The following frameworks are used in the implementation done by me -\n",
    "1. PyTorch\n",
    "2. Torchvision\n",
    "3. NumPy\n",
    "4. Matplotlib\n",
    "5. Seaborn\n",
    "6. Scikit-learn\n",
    "7. Sklearn.manifold\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49e5c6b",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Papers\n",
    "\n",
    "1. Chen, T., Kornblith, S., Norouzi, M., & Hinton, G. (2020). A Simple Framework for Contrastive Learning of Visual Representations. In International Conference on Machine Learning (ICML).\n",
    "\n",
    "2. He, K., Fan, H., Wu, Y., Xie, S., & Girshick, R. (2020). Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n",
    "\n",
    "\n",
    "### Videos and Talks\n",
    "\n",
    "1. Ting Chen: \"A Simple Framework for Contrastive Learning of Visual Representations\" - ICML 2020 Talk\n",
    "2. Yann LeCun: \"Self-Supervised Learning: The Dark Matter of Intelligence\" - Facebook AI Blog\n",
    "\n",
    "### LLM Tools\n",
    "\n",
    "1. Claude for managing the images html code in the blog."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
