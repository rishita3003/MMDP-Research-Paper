
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Motivation behind this project topic? &#8212; My Jupyter Book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'blog_contrastive_learning';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="#">
  
  
  
  
  
  
    <p class="title logo__title">My Jupyter Book</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1 current active">
                <a class="reference internal" href="#">
                    Motivation behind this project topic?
                </a>
            </li>
        </ul>
        
    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/blog_contrastive_learning.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Motivation behind this project topic?</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Motivation behind this project topic?</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#history-and-current-works">History and current works?</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#diving-deep-into-contrastive-learning-and-simclr-s-approach">Diving deep into Contrastive Learning and SimCLR’s approach</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#contrastive-loss-and-where-should-it-be-applied">Contrastive Loss and Where should it be applied?</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contrastive-loss-function">Contrastive Loss function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-s-the-temperature-parameter"><em>What’s the temperature parameter?</em></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#key-insights">Key Insights</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#algorithm-in-a-nutshell">Algorithm in a nutshell</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#results">Results</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#what-surprised-me">What surprised me!?</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#is-there-a-scope-for-improvement">Is there a scope for improvement??</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#want-to-see-some-results">Want to see some results?</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lets-take-a-look-at-a-pytorch-implementation-which-i-did-for-simclr"><em>Lets take a look at a Pytorch implementation which I did for SimCLR</em></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#results-on-downstream-task">Results on downstream task</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-evaluation">1. Linear Evaluation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#label-efficiency-evaluation">2. Label Efficiency evaluation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-the-representations">Visualizing the representations</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#tools">Tools</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#papers">Papers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#videos-and-talks">Videos and Talks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-tools">LLM Tools</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div style="display: flex; align-items: center; margin-bottom: 20px;">
  <img src="https://tse4.mm.bing.net/th?id=OIP.EhV6r5gkOCL2LvxAkFAnigAAAA&rs=1&pid=ImgDetMain" alt="SimCLR Logo" style="height: 60px; margin-right: 15px;">
  <div>
    <h2 style="margin: 0;">Look Twice, Learn Better : How SimCLR transformed Computer Vision</h2>
    <p style="margin: 0; color: #0066cc; font-size: 22px;"><em>Rishita Agarwal</em></p>
  </div>
</div>
<iframe width="560" height="315" src="https://youtu.be/muJMTto75qE " title="SimCLR Explanation" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<section id="motivation-behind-this-project-topic">
<h1>Motivation behind this project topic?<a class="headerlink" href="#motivation-behind-this-project-topic" title="Link to this heading">#</a></h1>
<p>The growth of digital images has created a havoc in machine learning, we have a lot of visual data, yet supervised learning approaches act as a bottleneck due to the lack of high quality labeled data. This is one of the major challenges in computer vision today.</p>
<p>One of the solution which can be proposed for this can be manual annotation of the data, but manual labelling is expensive, time consuming and many times impractical at large scale. Some times it may also require expert knowledge in a particular domain.</p>
<p><strong>Unsupervised learning</strong> offers a more promising path by leveraging the large amount of unlabeled data available. Though, it is not that easy to produce results as good as supervised learning. Some of the key requirements are -</p>
<ol class="arabic simple">
<li><p>The learned representation should be <strong>generalizable</strong> to diverse downstream tasks as well.</p></li>
<li><p>The approach should <strong>scale computationally</strong> with larger datasets and model sizes.</p></li>
<li><p>Features should capture <strong>meaningful semantic</strong> information.</p></li>
</ol>
<div style="display: flex; justify-content: center;">
    <img src="https://www.edushots.com/upload/articles-images/b35a6ab4259fcd2fa572cc62333ac5ec15371617.jpg" alt="SimCLR Framework" width="45%" style="margin-right: 10px;"/>
    <img src="https://media.geeksforgeeks.org/wp-content/uploads/20231213175718/Self-660.png" alt="SimCLR Results" width="45%"/>
</div>
<p style="text-align: center;"><em>Figure: Unsupervised Learning (left) and Self Supervised Learning (right)</em></p>
<p>SimCLR addressed these requirements in a simplistic manner, still managing to achieve state-of-the-art results, grabing my attention to this topic. The idea was simple yet innovative, which actually was derived from the essential components of existing methods.</p>
</section>
<section id="history-and-current-works">
<h1>History and current works?<a class="headerlink" href="#history-and-current-works" title="Link to this heading">#</a></h1>
<p>The initial methods relating to <strong>self supervised visual representations</strong> led to this exploration. Let me talk more about these methods.</p>
<p>It all started with <strong>handcrafted pretext</strong> tasks like predicting image rotation or colorizing grayscale images. These methods were a good start to the finding a good representation, but it was noticed that the learnt representations were more specific to the pretext task rather than being general purpose.</p>
<p>Then, approaches like <strong>InstDisc and CPC</strong> introduced the concept of contrastive objectives (Fig. given below) but still relied on complex architectures or memory banks to store representations.</p>
<div style="display: flex; justify-content: center;">
    <img src="https://insights.willogy.io/assets/static/contrastive_learning_intuition.42db587.208b1cf168018c6226966d0407c62134.jpg" alt="Contrastive Learning Concept" width="50%"/>
</div>
<p style="text-align: center;"><em>Figure: Visualization of contrastive learning approach</em></p>
<p>Finally <strong>SimCLR</strong> simplied the approach of contrastive learning dramatically, showing that with right data augmentation, loss function and projection head, superior results with a straightforward framework could be achieved.
Even after SimCLR more methods based on its insights were built, like, MoCo v2, SimSiam etc. to further improve the self supervision, trying to reduce the need for negative examples. CLIP and CLAP are 2 multimodal approaches to Contrastive learning, applying CL to image-text and audio-text pairs. I will give a basic introduction to these methods towards the end of this blog.</p>
</section>
<section id="diving-deep-into-contrastive-learning-and-simclr-s-approach">
<h1>Diving deep into Contrastive Learning and SimCLR’s approach<a class="headerlink" href="#diving-deep-into-contrastive-learning-and-simclr-s-approach" title="Link to this heading">#</a></h1>
<p><strong>Contrastive learning</strong> is an approach that learns representations by comparing similar and dissimilar samples. The fundamental idea behind this concept is: Similar items (“Positive”) should be closer together and dissimilar items (“Negative”) should be further apart.</p>
<p>It is a self supervised technique which learns meaningful representation without any explicitly labeled data.</p>
<p><strong>SimCLR (Simple Framework for Contrastive Learning of Visual Representation) Framework</strong></p>
<p>This framework consists of 4 major components -</p>
<ol class="arabic simple">
<li><p><strong>Data Augmentation Module</strong> - In this module, each input image goes through stochastic augmentation twice, to generate 2 different views.</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="n">augmentation</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="n">x̃ᵢ</span>
<span class="n">X</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="n">augmentation</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="n">x̃ⱼ</span>
</pre></div>
</div>
<p>The augmentation sequence is as follows :</p>
<ul class="simple">
<li><p><strong>Random Cropping</strong> - After cropping the image randomly, it should again be resized to original size.</p></li>
<li><p><strong>Random color distortion</strong> - Include color dropping, brightness, contrast etc.</p></li>
<li><p><strong>Random Gaussian blur</strong> - Gaussian function to smooth an image. The effect is similar to viewing the image through a translucent screen, creating a hazy appearance by reducing image noise and detail.</p></li>
</ul>
<p>The most important combination of transformation is that of cropping (spatial transformation) and color distortion (appearance transformation). A good reason for this is, that without color distortion the networks can exploit the shortcut of matching color histograms, rather than actually learning semantic features.</p>
<div style="display: flex; justify-content: center;">
    <img src="https://tse1.mm.bing.net/th?id=OIP.l9m-_lWHc2iopae_sHtdUwHaDM&rs=1&pid=ImgDetMain" alt="Data Augmentation" width="45%" style="margin-right: 10px;"/>
    <img src="https://img-blog.csdnimg.cn/20201013134203801.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h6MTMwODU3OTM0MA==,size_16,color_FFFFFF,t_70#pic_center" alt="Data Augmentation" width="45%" style="margin-right: 10px;"/>
</div>
<p style="text-align: center;"><em>Figure: Data Augmentation Example (left) and Composition of Augmentation Techniques (right)</em></p>
<ol class="arabic simple" start="2">
<li><p><strong>Base Encoder Network</strong></p></li>
</ol>
<p>SimCLR employs a standard CNN (ResNet architecture) as its base encoder. Given an augmented image x̃, the encoder generates a representation vector -</p>
<p><code class="docutils literal notranslate"><span class="pre">h</span> <span class="pre">=</span> <span class="pre">f(x̃)</span> <span class="pre">=</span> <span class="pre">ResNet(x̃)</span></code></p>
<p>where h ∈ ℝᵈ is the output after the average pooling layer.</p>
<ol class="arabic simple" start="3">
<li><p><strong>Projection Head</strong>
This head maps the representation h to a space z, where contrastive loss is applied -</p></li>
</ol>
<p><code class="docutils literal notranslate"><span class="pre">z</span> <span class="pre">=</span> <span class="pre">g(h)</span> <span class="pre">=</span> <span class="pre">W₂σ(W₁h)</span></code></p>
<p>Where σ is a ReLU nonlinearity, and W₁ and W₂ are learnable weights.
In this research paper, they have made a relatively simple MLP with one hidden layer as the projection head.</p>
</section>
<section id="contrastive-loss-and-where-should-it-be-applied">
<h1>Contrastive Loss and Where should it be applied?<a class="headerlink" href="#contrastive-loss-and-where-should-it-be-applied" title="Link to this heading">#</a></h1>
<p>It was noticed that applying the contrastive loss directly on the representations h hurts their quality for downstream tasks. The projection head allows the model to discard information that may be useful for the downstream task but not for the contrastive task.</p>
<p>Crucially, after training is complete, the projection head is discarded, and the <strong>representation h is used for downstream tasks.</strong></p>
<section id="contrastive-loss-function">
<h2>Contrastive Loss function<a class="headerlink" href="#contrastive-loss-function" title="Link to this heading">#</a></h2>
<p>A special form of contrastive loss called NT-Xent (Normalized Temperature scaled Cross Entropy Loss) is used. For a positive pair (i, j) :</p>
<p><code class="docutils literal notranslate"><span class="pre">ℓᵢ,ⱼ</span> <span class="pre">=</span> <span class="pre">-log(exp(sim(zᵢ,zⱼ)/τ)</span> <span class="pre">/</span> <span class="pre">Σₖ₌₁²ᴺ</span> <span class="pre">1[k≠i]exp(sim(zᵢ,zₖ)/τ))</span></code></p>
<p>Where:</p>
<ul class="simple">
<li><p>sim(u,v) = u·v/‖u‖‖v‖ is the cosine similarity</p></li>
<li><p>τ is a temperature parameter that controls the concentration level of the distribution.</p></li>
<li><p>The sum is over all 2N examples in the batch (including the other augmented views)</p></li>
</ul>
<p>This loss effectively treats each augmented image in the batch as a single positive example (the other augmented view of the same image) and 2N-2 negative examples (all other augmented images)</p>
</section>
<section id="what-s-the-temperature-parameter">
<h2><em>What’s the temperature parameter?</em><a class="headerlink" href="#what-s-the-temperature-parameter" title="Link to this heading">#</a></h2>
<p>Hmmm… Good question. Since we have already L2 normalized our embeddings z1 and z2, why do we need this temperature scaling parameter? The temperature parameter (τ) essentially controls how “peaky” or “smooth” the distribution becomes.</p>
<ol class="arabic simple">
<li><p><strong>Lower temperature</strong> - Amplifies the differences between the similarity scores and makes the model more certain about its choices. Eg. τ = 0.1, would tend to give sharper distinctions between positive and negative pairs and model would focus more on hardest negative pairs.</p></li>
<li><p><strong>Higher temperature</strong> - Smooths out the differences between similarity scores and reates a more uniform distribution. It allows the model to consider a broader range of negative examples</p></li>
</ol>
<p>So, normalization is needed to ensure all embeddings lie on the unit hypersphere (-1 to 1 for cosine similarity) and temperatures takes care of the concentration of attention on the positive and negative pairs.</p>
<div style="display: flex; justify-content: center;">
    <img src="https://sthalles.github.io/assets/contrastive-self-supervised/cover.png" alt="SimCLR Framework" width="70%"/>
</div>
<p style="text-align: center;"><em>Figure: SimCLR Framework</em></p>
</section>
</section>
<section id="key-insights">
<h1>Key Insights<a class="headerlink" href="#key-insights" title="Link to this heading">#</a></h1>
<p>The major points which can be reflected from this paper are described in the section.
Based on the extensive ablation studies which were conducted -</p>
<ol class="arabic simple">
<li><p><strong>Data augmentation composition is critical</strong> - It was observed that following a specific sequence and combination of augmentations enhanced the results. Random cropping + color distortion provides significant benefit.</p></li>
<li><p><strong>Normalized embeddings and temperature scaling matters</strong> - Temperature effectively controls the importance of difficult negative examples.</p></li>
<li><p><strong>Contrastive Learning benefits from larger batch sizes</strong> - Larger batches (up to 8192) continue to improve performance as they provide more negative examples.</p></li>
<li><p><strong>Longer training improves results</strong> - SimCLR benefits more from training for longer periods than supervised counterparts.</p></li>
<li><p><strong>Non linear projection head is crucial</strong> - A linear projection is better than no projection, but non linear projection works even better, improving accuracy 3-4%.</p></li>
<li><p><strong>Unsupervised learning benefits more from model scaling</strong> - As the model size increases, the gap between supervised and self supervised models shrinks.</p></li>
</ol>
</section>
<section id="algorithm-in-a-nutshell">
<h1>Algorithm in a nutshell<a class="headerlink" href="#algorithm-in-a-nutshell" title="Link to this heading">#</a></h1>
<ol class="arabic simple">
<li><p>Sample a minibatch of N images</p></li>
<li><p>For each image x, generate two augmented versions x̃ᵢ and x̃ⱼ</p></li>
<li><p>Compute representations <code class="docutils literal notranslate"><span class="pre">h</span> <span class="pre">=</span> <span class="pre">f(x̃)</span></code> and projections <code class="docutils literal notranslate"><span class="pre">z</span> <span class="pre">=</span> <span class="pre">g(h)</span></code> for all 2N augmented images</p></li>
<li><p>For each positive pair, compute the contrastive loss using all other 2N-2 augmented examples as negatives</p></li>
<li><p>Update the networks f and g to minimize the loss</p></li>
<li><p>After training, discard the projection head g and use the encoder f and representation h for downstream tasks</p></li>
</ol>
<div style="display: flex; justify-content: center;">
    <img src="./blog_images/SimCLR.png" alt="SimCLR Framework" width="30%"/>
</div>
<p style="text-align: center;"><em>Figure: SimCLR Simple Example</em></p>
</section>
<section id="results">
<h1>Results<a class="headerlink" href="#results" title="Link to this heading">#</a></h1>
<p>The simplicity and effectiveness of SimCLR made it a pivotal development in self-supervised learning. Some notable achievements:</p>
<ul class="simple">
<li><p>Linear classifiers trained on SimCLR representations achieved <strong>76.5% top-1</strong> accuracy on ImageNet, a <em>7% relative improvement</em> over previous state-of-the-art methods</p></li>
<li><p>With just 1% of labeled ImageNet data, fine-tuned SimCLR achieved <strong>85.8% top-5</strong> accuracy</p></li>
<li><p>SimCLR outperformed supervised pre-training on multiple transfer learning tasks</p></li>
</ul>
<p>Perhaps most importantly, SimCLR showed that with the right components, a simple approach to contrastive learning could outperform more complex methods, setting a new direction for self-supervised learning research.</p>
<div style="display: flex; justify-content: center;">
    <img src="./blog_images/results.png" alt="SimCLR Framework" width="50%"/>
</div>
<p style="text-align: center;"><em>Figure: SimCLR results on transfer learning tasks</em></p>
</section>
<section id="what-surprised-me">
<h1>What surprised me!?<a class="headerlink" href="#what-surprised-me" title="Link to this heading">#</a></h1>
<p>After going through the results and concept in depth, I found several aspects which surprised me -</p>
<ol class="arabic simple">
<li><p><strong>The power of simple data augmentation:</strong> A carefully composed and sequential data augmentations provided such effective contrastive tasks without any complex architecture. Random cropping + color distortion in this case.</p></li>
</ol>
<p>A question one might have is - <strong>Why Random Cropping works??</strong></p>
<div style="display: flex; justify-content: center;">
    <img src="https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial17/crop_views.svg?raw=1" alt="SimCLR Framework" width="50%"/>
</div>
<p style="text-align: center;"><em>Figure: SimCLR Simple Example</em></p>
<p>When performing randomly cropping and resizing, we can distinguish between two situations: (a) cropped image A provides a local view of cropped image B, or (b) cropped images C and D show neighboring views of the same image. While situation (a) requires the model to learn some sort of scale invariance to make crops A and B similar in latent space, situation (b) is more challenging since the model needs to recognize an object beyond its limited view.</p>
<ol class="arabic simple" start="2">
<li><p><strong>Projection head paradox:</strong> It was observed through experiments that the projection head works to remove information that may be useful for downstream tasks, which worked in a fascinating way as then the result of projection head could be used for contrastive loss.</p></li>
<li><p><strong>Transfer learning performance:</strong> This aspect highlights the major aim of the representations produced by SimCLR. It was observed that the SimCLR representations performed better than supervised ImageNet pretraining was completely unexpected and important aspect. This removed the issue of task specific representation of self supervision.</p></li>
</ol>
<div style="display: flex; justify-content: center;">
    <img src="https://th.bing.com/th/id/OIP.tnTNLLEZNfrvlr_kkoBezwHaFO?rs=1&pid=ImgDetMain" alt="Results" width="50%"/>
</div>
<p style="text-align: center;"><em>Figure: SimCLR Results</em></p>
</section>
<section id="is-there-a-scope-for-improvement">
<h1>Is there a scope for improvement??<a class="headerlink" href="#is-there-a-scope-for-improvement" title="Link to this heading">#</a></h1>
<p>Even after all these fascinating aspects of this architecture, there still remains several areas for improvement:</p>
<ol class="arabic simple">
<li><p><strong>Computational efficiency:</strong> SimCLR requires large batch sizes and long training times for better performance, which makes it computationally expensive. Although, further works like <em>MoCo</em> address this issue using momentum encoders and memory banks.</p></li>
<li><p><strong>Negative sample dependency:</strong> The need for many negative examples require large batch sizes. Recent approaches like <em>BYOL and SimSiam</em> show that its possible to learn without explicit negatives.</p></li>
<li><p><strong>Multimodality:</strong> While SimCLR focusses on visual representations, <em>CLIP and CLAP</em> architectures extend it to multiple modalities, which yielded even richer representations.</p></li>
<li><p><strong>Exploration:</strong> Exploration of more achitectures other than ResNet and actual reasoning of why certain data augmentation compositions work better than other, would help develop self supervised learning more.</p></li>
</ol>
</section>
<section id="want-to-see-some-results">
<h1>Want to see some results?<a class="headerlink" href="#want-to-see-some-results" title="Link to this heading">#</a></h1>
<section id="lets-take-a-look-at-a-pytorch-implementation-which-i-did-for-simclr">
<h2><em>Lets take a look at a Pytorch implementation which I did for SimCLR</em><a class="headerlink" href="#lets-take-a-look-at-a-pytorch-implementation-which-i-did-for-simclr" title="Link to this heading">#</a></h2>
<p>I implemented the SimCLR framework on a sample of <em>CIFAR10</em> (10000 images) for training the model, with <em>ResNet18</em> as the base encoder and a simple <em>MLP layer</em> for projection head.</p>
<p>Each of the image is 32x32 color image and there are 10 different classes of images. The classes are -
[Airplane, Automobile, Bird, Car, Deer, Dog, Frog, Horse, Ship]</p>
<p>The code below applies data augmentation (Random Crop + Color distortion + Gaussian Blur) twice to the image -</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define data augmentation pipeline</span>
<span class="k">class</span><span class="w"> </span><span class="nc">TwoCropTransform</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Create two crops of the same image&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">transform</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transform</span> <span class="o">=</span> <span class="n">transform</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">x</span><span class="p">)]</span>

<span class="c1"># Define data augmentations as per SimCLR paper</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_simclr_pipeline_transform</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return a set of data augmentation transformations as described in the SimCLR paper.&quot;&quot;&quot;</span>
    <span class="n">color_jitter</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ColorJitter</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>
    
    <span class="n">data_transforms</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">RandomResizedCrop</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">RandomApply</span><span class="p">([</span><span class="n">color_jitter</span><span class="p">],</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.8</span><span class="p">),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">RandomGrayscale</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">GaussianBlur</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">0.1</span> <span class="o">*</span> <span class="n">size</span><span class="p">)),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span> <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>
    <span class="p">])</span>
    <span class="k">return</span> <span class="n">data_transforms</span>
</pre></div>
</div>
<p>On applying the above augmentation, I got 2N images for N images in my dataset. <strong>Positive pairs</strong> are those pair of images which we got from the same original image.</p>
<p>The following are the various data augmentations on an image -</p>
<div style="display: flex; justify-content: center;">
    <img src="./blog_images/imp1.png" alt="SimCLR Framework" width="50%"/>
</div>
<p style="text-align: center;"><em>Figure: CIFAR10 image with augmentations</em></p>
<p>Then I defined the NT-Xent Loss (aiming to bring the positive pairs closer and the negative pairs further apart) -</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z_i</span><span class="p">,</span> <span class="n">z_j</span><span class="p">):</span>
        <span class="c1"># Get actual batch size (might be smaller than self.batch_size for last batch)</span>
        <span class="n">current_batch_size</span> <span class="o">=</span> <span class="n">z_i</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="c1"># Create mask dynamically based on actual batch size</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_mask</span><span class="p">(</span><span class="n">current_batch_size</span><span class="p">)</span>
        
        <span class="c1"># Concatenate the representations from the two augmentations</span>
        <span class="n">representations</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">z_i</span><span class="p">,</span> <span class="n">z_j</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="c1"># Compute similarity matrix - more memory efficient approach</span>
        <span class="n">similarity_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">representations</span><span class="p">,</span> <span class="n">representations</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>
        
        <span class="c1"># Normalize the similarity matrix</span>
        <span class="n">sim_i_j</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">similarity_matrix</span><span class="p">,</span> <span class="n">current_batch_size</span><span class="p">)</span>
        <span class="n">sim_j_i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">similarity_matrix</span><span class="p">,</span> <span class="o">-</span><span class="n">current_batch_size</span><span class="p">)</span>
        
        <span class="n">positives</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">sim_i_j</span><span class="p">,</span> <span class="n">sim_j_i</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="c1"># Remove diagonal (self-similarity)</span>
        <span class="n">mask_samples_from_same_repr</span> <span class="o">=</span> <span class="o">~</span><span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">current_batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">negatives</span> <span class="o">=</span> <span class="n">similarity_matrix</span><span class="p">[</span><span class="n">mask_samples_from_same_repr</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">current_batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Scale by temperature</span>
        <span class="n">positives</span> <span class="o">=</span> <span class="n">positives</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span>
        <span class="n">negatives</span> <span class="o">=</span> <span class="n">negatives</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span>
        
        <span class="c1"># Create logits and compute loss</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">positives</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">negatives</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">current_batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">current_batch_size</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">loss</span>

</pre></div>
</div>
<p>The similarity between the pairs of vectors is in the loss function. Thus, making use of the negative samples.</p>
<p>The code below is the final code for training SimCLR -</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="k">def</span><span class="w"> </span><span class="nf">train_simclr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">epochs</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="c1"># Add debugging info</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Starting training with </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span><span class="si">}</span><span class="s2"> batches per epoch&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Actual batch size: </span><span class="si">{</span><span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">batch_count</span> <span class="o">=</span> <span class="mi">0</span>
        
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">images</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="c1"># Get the two augmented views of the same batch</span>
                <span class="p">(</span><span class="n">x_i</span><span class="p">,</span> <span class="n">x_j</span><span class="p">)</span> <span class="o">=</span> <span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                
                <span class="c1"># Add progress info</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Processing batch </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                
                <span class="c1"># Debug info for first batch</span>
                <span class="k">if</span> <span class="n">epoch</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input shape: </span><span class="si">{</span><span class="n">x_i</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                
                <span class="n">x_i</span><span class="p">,</span> <span class="n">x_j</span> <span class="o">=</span> <span class="n">x_i</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">x_j</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

                <span class="c1"># Zero the gradients</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
                
                <span class="c1"># Forward pass for both augmentations with added memory efficiency</span>
                <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">autocast</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()):</span>
                    <span class="n">z_i</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_i</span><span class="p">)</span>
                    <span class="n">z_j</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_j</span><span class="p">)</span>
                    
                    <span class="c1"># Compute loss</span>
                    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">z_i</span><span class="p">,</span> <span class="n">z_j</span><span class="p">)</span>
                
                <span class="c1"># Backward pass</span>
                <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
                
                <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="n">batch_count</span> <span class="o">+=</span> <span class="mi">1</span>
                
                <span class="c1"># Free up memory</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="kc">None</span>
                
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error in batch </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">continue</span>
        
        <span class="c1"># Record epoch loss</span>
        <span class="n">epoch_loss</span> <span class="o">=</span> <span class="n">running_loss</span> <span class="o">/</span> <span class="n">batch_count</span> <span class="k">if</span> <span class="n">batch_count</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">)</span>
        <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch_loss</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">epochs</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">epoch_loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Processed </span><span class="si">{</span><span class="n">batch_count</span><span class="si">}</span><span class="s2"> batches&quot;</span><span class="p">)</span>
        
        <span class="c1"># Save model checkpoint once at the end</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="o">==</span> <span class="n">epochs</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="sa">f</span><span class="s2">&quot;simclr_final.pt&quot;</span><span class="p">)</span>
    
    <span class="c1"># Plot loss curve</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;SimCLR Training Loss&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;simclr_loss.png&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Could not plot loss curve: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">losses</span>

</pre></div>
</div>
<p>Due to restricted time and computational resources, I took a subset of the data and also ran for a limited batch_size = 256 and 100 epochs. I took a temperature of 0.5 for a balanced check for positive and negative samples.</p>
<p>Finally, training loss was a decreasing curve over the epochs:</p>
<div style="display: flex; justify-content: center;">
    <img src="./final_results/simclr_loss.png" alt="SimCLR Framework" width="50%"/>
</div>
<p style="text-align: center;"><em>Figure: Training Loss of SimCLR over the 100 epochs</em></p></section>
<section id="results-on-downstream-task">
<h2>Results on downstream task<a class="headerlink" href="#results-on-downstream-task" title="Link to this heading">#</a></h2>
<section id="linear-evaluation">
<h3>1. Linear Evaluation<a class="headerlink" href="#linear-evaluation" title="Link to this heading">#</a></h3>
<p>In this downstream task, I performed the linear evaluation task. Lets take a look at the steps involved in this:</p>
<ul class="simple">
<li><p>Load your pre-trained SimCLR model</p></li>
<li><p>Freeze the encoder (so the weights don’t change)</p></li>
<li><p>Extract features from all your training and test images</p></li>
<li><p>Train a simple linear classifier on top of these features</p></li>
<li><p>Evaluate performance on the test set</p></li>
</ul>
<p>The implementation extracts features from both training and test datasets in a single pass through the frozen encoder, significantly accelerating the evaluation process compared to end-to-end fine-tuning. The linear classifier itself is intentionally simple—just a single fully-connected layer mapping the 512-dimensional feature vectors to the 10 CIFAR-10 classes—to ensure that classification performance genuinely reflects the quality of the learned representations rather than the power of the classifier.
This is the most common way to evaluate self-supervised representations because it tests if your features are linearly separable.</p>
<div style="display: flex; justify-content: center;">
    <img src="./final_results/training_curves.png" alt="Linear Evaluation" width="45%" style="margin-right: 10px;"/>
    <img src="./final_results/confusion_matrix.png" alt="Confusion Matrix" width="45%" style="margin-right: 10px;"/>
</div>
<p style="text-align: center;"><em>Figure: Linear Evaluation Training Curves (left) and Confusion Matrix (right)</em></p>
<p>The above plots show that there is confusion between classes 0 and 8 (plane and ship) and classes 1 and 9 (car and truck). These can be as these are visually similar classes (similar shapes and features).</p>
<p>The confusion patterns suggest that SimCLR has learned representations that capture some semantic similarities but still struggles with fine-grained distinctions between visually similar categories. This might be due to lack of training data as unsupervised learning needs more data in comparison to a supervised learning approach. Training on more epochs or increasing the batch size might also help improve the performance as then more negative samples can be compared.</p>
</section>
<section id="label-efficiency-evaluation">
<h3>2. Label Efficiency evaluation<a class="headerlink" href="#label-efficiency-evaluation" title="Link to this heading">#</a></h3>
<p>One of the big advantages of self-supervised learning is that it can work well with less labeled data.
I implemented a <code class="docutils literal notranslate"><span class="pre">evaluate_with_less_data()</span></code> function that tests how well your model performs when trained with only a small percentage of labeled data (1%, 10%, 25%, etc.)
The following is a plot showing how accuracy changes as you add more labeled data-</p>
<div style="display: flex; justify-content: center;">
    <img src="./final_results/label_efficiency.png" alt="Label Efficiency" width="55%" style="margin-right: 10px;"/>
</div>
<p style="text-align: center;"><em>Figure: Test accuracy on label efficiency task</em></p>
<p>The above plot show that there is a steep performance jump from <strong>1% to 10% labeled data</strong>. This accuraccy forms a plateau after 50% labelled data.</p>
<p>The accuracy is not more than 50% right now, due to many factors including less training data used (only 10000 images) and low batch size and epochs for training.</p>
<p>Still, the nature of the curve depicts that for this 50% accuracy only 10% of the lavelled data was enough.
This confirms one of the core value propositions of contrastive learning methods like SimCLR - <em>they can learn useful representations that transfer well even with limited labeled data.</em></p>
<p>The relatively high performance <strong>(43%)</strong> even with just <strong>1% labeled data</strong> demonstrates the effectiveness of the knowledge transfer from self-supervised pre-training to downstream tasks.</p>
</section>
</section>
</section>
<section id="visualizing-the-representations">
<h1>Visualizing the representations<a class="headerlink" href="#visualizing-the-representations" title="Link to this heading">#</a></h1>
<p>The visualization shows how, after training, semantically similar images cluster together in the representation space, even though the model never saw class labels. This organization emerges purely from the contrastive learning objective.</p>
<p>Given below is the t-SNE visualization of the embeddings -</p>
<div style="display: flex; justify-content: center;">
    <img src="./final_results/tsne_embeddings.png" alt="TSNE Visualization" width="55%" style="margin-right: 10px;"/>
</div>
<p style="text-align: center;"><em>t-SNE Visualization of the embeddings</em></p><p>The entire implementation is available at <a class="github reference external" href="https://github.com/rishita3003/MMDP-Research-Paper/blob/main/training_evaluation.ipynb">rishita3003/MMDP-Research-Paper</a></p>
</section>
<section id="tools">
<h1>Tools<a class="headerlink" href="#tools" title="Link to this heading">#</a></h1>
<p>The following frameworks are used in the implementation done by me -</p>
<ol class="arabic simple">
<li><p>PyTorch</p></li>
<li><p>Torchvision</p></li>
<li><p>NumPy</p></li>
<li><p>Matplotlib</p></li>
<li><p>Seaborn</p></li>
<li><p>Scikit-learn</p></li>
<li><p>Sklearn.manifold</p></li>
</ol>
</section>
<section id="references">
<h1>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h1>
<section id="papers">
<h2>Papers<a class="headerlink" href="#papers" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Chen, T., Kornblith, S., Norouzi, M., &amp; Hinton, G. (2020). A Simple Framework for Contrastive Learning of Visual Representations. In International Conference on Machine Learning (ICML).</p></li>
<li><p>He, K., Fan, H., Wu, Y., Xie, S., &amp; Girshick, R. (2020). Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.</p></li>
</ol>
</section>
<section id="videos-and-talks">
<h2>Videos and Talks<a class="headerlink" href="#videos-and-talks" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Ting Chen: “A Simple Framework for Contrastive Learning of Visual Representations” - ICML 2020 Talk</p></li>
<li><p>Yann LeCun: “Self-Supervised Learning: The Dark Matter of Intelligence” - Facebook AI Blog</p></li>
</ol>
</section>
<section id="llm-tools">
<h2>LLM Tools<a class="headerlink" href="#llm-tools" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Claude for managing the images html code in the blog.</p></li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Motivation behind this project topic?</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#history-and-current-works">History and current works?</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#diving-deep-into-contrastive-learning-and-simclr-s-approach">Diving deep into Contrastive Learning and SimCLR’s approach</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#contrastive-loss-and-where-should-it-be-applied">Contrastive Loss and Where should it be applied?</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contrastive-loss-function">Contrastive Loss function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-s-the-temperature-parameter"><em>What’s the temperature parameter?</em></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#key-insights">Key Insights</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#algorithm-in-a-nutshell">Algorithm in a nutshell</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#results">Results</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#what-surprised-me">What surprised me!?</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#is-there-a-scope-for-improvement">Is there a scope for improvement??</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#want-to-see-some-results">Want to see some results?</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lets-take-a-look-at-a-pytorch-implementation-which-i-did-for-simclr"><em>Lets take a look at a Pytorch implementation which I did for SimCLR</em></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#results-on-downstream-task">Results on downstream task</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-evaluation">1. Linear Evaluation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#label-efficiency-evaluation">2. Label Efficiency evaluation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-the-representations">Visualizing the representations</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#tools">Tools</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#papers">Papers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#videos-and-talks">Videos and Talks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-tools">LLM Tools</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book community
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>